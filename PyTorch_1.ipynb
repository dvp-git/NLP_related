{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learnings related to PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coversion to one-Hot using \n",
    " - Torch: `nn.functional.one_hot`\n",
    " - Pandas: `.get_dummies()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch OneHot tensor([[0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 1, 0, 0, 0]])\n",
      "Numpy One-Hot Cnverted  tensor([[1, 0, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [1, 0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "y = 1\n",
    "num_classes = 3\n",
    "data_ = pd.DataFrame({\n",
    "    'y':[1,3,4,2,3,1,4,2,3,4,1]\n",
    "})\n",
    "\n",
    "# Create the one-hot encoded vector using NumPy\n",
    "one_hot_numpy = np.array([0, 1, 0])\n",
    "\n",
    "# Create the one-hot encoded vector using PyTorch\n",
    "one_hot_pytorch = F.one_hot(torch.tensor(y),num_classes=3)\n",
    "#print(one_hot_numpy)\n",
    "#print(one_hot_pytorch)\n",
    "\n",
    "print(\"Torch OneHot\",F.one_hot(torch.tensor(data_['y'])))\n",
    "\n",
    "print(\"Numpy One-Hot Cnverted \",torch.tensor(pd.get_dummies(data_['y']).to_numpy().astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "print(criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.0619, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "y = [2]\n",
    "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
    "\n",
    "# Create a one-hot encoded vector of the label y\n",
    "one_hot_label = F.one_hot(torch.tensor(y), scores.shape[1])\n",
    "\n",
    "# Create the cross entropy loss function : As object of the CrossEntropyLoss class\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "\n",
    "# Calculate the cross entropy loss\n",
    "loss = criterion(scores.double(),one_hot_label.double())    \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method in the nn.CrossEntropyLoss class that is being called is forward(). When you call criterion(scores.double(), one_hot_label.double()), it is equivalent to calling criterion.forward(scores.double(), one_hot_label.double()). The forward() method performs the actual computation of the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.0619, dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the functinal nn.functional.cross_entropy function\n",
    "F.cross_entropy(scores.double(),one_hot_label.double())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample MSE using pyTorch and Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat 10\n",
      "y 1\n",
      "mse_numpy 81.0\n",
      "mse_pytorch tensor(81.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_hat = np.array(10)  # 10 outputs\n",
    "y = np.array(1)       \n",
    "\n",
    "print(\"y_hat\",y_hat)\n",
    "print(\"y\",y)\n",
    "\n",
    "# Calculate the MSELoss using NumPy\n",
    "mse_numpy = np.mean((y.astype(float)-y_hat.astype(float))**2)\n",
    "\n",
    "\n",
    "\n",
    "# Create the MSELoss function\n",
    "criterion =  nn.MSELoss()\n",
    "\n",
    "# Calculate the MSELoss using the created loss function\n",
    "mse_pytorch = criterion(torch.tensor(y_hat).float(), torch.tensor(y).float())\n",
    "\n",
    "\n",
    "print(\"mse_numpy\",mse_numpy)\n",
    "print(\"mse_pytorch\",mse_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Neural Nework : Regression\n",
    "```python\n",
    "# Number of epochs to train\n",
    "for n in range(num_epochs):\n",
    "  for data in dataloader:\n",
    "    # Reset gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "    feature, target = data\n",
    "    prediction = model(feature)    \n",
    "    loss = criterion(prediction, target)   # MSE for regression, CrossEntropy for classification etc   \n",
    "    #Compute the gradients of parameters (weights, bias)\n",
    "    loss.backward()         \n",
    "    # Update the weigths and bias -> W = W - dloss/dW , B = B - dloss/dB\n",
    "    optimizer.step()\n",
    "\n",
    "# Show the model weights and bias ( parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A View of the internal weights and Bias.\n",
    "`Counting Learnable parameters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,2,3,4,5,6,7,8],[1,3,5,7,2,4,6,8]])  # 2 inputs, 8 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(8,4), nn.Linear(4,2)) # Hidden 1 : 4 layers, Hidden 2 : 2 layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5136,  0.0814],\n",
       "        [ 0.8048, -0.3666]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x.float())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.children of Sequential(\n",
       "  (0): Linear(in_features=8, out_features=4, bias=True)\n",
       "  (1): Linear(in_features=4, out_features=2, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights 0:  Parameter containing:\n",
      "tensor([[-0.2021, -0.2454,  0.0112,  0.0805,  0.2412,  0.1939, -0.2667,  0.0729],\n",
      "        [ 0.2147, -0.3125, -0.1370,  0.0931,  0.3466, -0.3268,  0.3312, -0.2827],\n",
      "        [-0.1324,  0.1452,  0.0364, -0.2102,  0.1515, -0.3330,  0.3410, -0.0280],\n",
      "        [ 0.3400,  0.0100, -0.1442, -0.2333,  0.2605, -0.1619,  0.1231, -0.0291]],\n",
      "       requires_grad=True)\n",
      "Bias 0: Parameter containing:\n",
      "tensor([-0.0342, -0.0171, -0.3411,  0.2428], requires_grad=True)\n",
      "\n",
      "Weights 1:  Parameter containing:\n",
      "tensor([[ 0.3980, -0.4980,  0.1079, -0.1080],\n",
      "        [-0.2084,  0.2341, -0.0214,  0.2514]], requires_grad=True)\n",
      "Bias 1: Parameter containing:\n",
      "tensor([-0.0673,  0.3301], requires_grad=True)\n",
      "\n",
      "Number of weight params 72\n",
      "Number of bias params 10\n",
      "Number of learnable parameters 82\n"
     ]
    }
   ],
   "source": [
    "count_weight = 0\n",
    "count_bias = 0\n",
    "\n",
    "for i,param in enumerate(model.children()):  # or param in model.parameters()\n",
    "    print(f\"Weights {i}: \", param.weight)\n",
    "    print(f\"Bias {i}:\", param.bias)\n",
    "    print()\n",
    "    i += 1\n",
    "    count_weight += count_weight + param.weight.numel()\n",
    "    count_bias += count_bias + param.bias.numel()\n",
    "\n",
    "print(\"Number of weight params\",count_weight)\n",
    "print(\"Number of bias params\",count_bias)\n",
    "print(\"Number of learnable parameters\",count_weight + count_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight Parameter containing:\n",
      "tensor([[-0.2021, -0.2454,  0.0112,  0.0805,  0.2412,  0.1939, -0.2667,  0.0729],\n",
      "        [ 0.2147, -0.3125, -0.1370,  0.0931,  0.3466, -0.3268,  0.3312, -0.2827],\n",
      "        [-0.1324,  0.1452,  0.0364, -0.2102,  0.1515, -0.3330,  0.3410, -0.0280],\n",
      "        [ 0.3400,  0.0100, -0.1442, -0.2333,  0.2605, -0.1619,  0.1231, -0.0291]],\n",
      "       requires_grad=True)\n",
      "0.bias Parameter containing:\n",
      "tensor([-0.0342, -0.0171, -0.3411,  0.2428], requires_grad=True)\n",
      "1.weight Parameter containing:\n",
      "tensor([[ 0.3980, -0.4980,  0.1079, -0.1080],\n",
      "        [-0.2084,  0.2341, -0.0214,  0.2514]], requires_grad=True)\n",
      "1.bias Parameter containing:\n",
      "tensor([-0.0673,  0.3301], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name,param in model.named_parameters():  # or param in model.parameters()\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data from datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Create a dataset from the two generated tensors\n",
    "from torch.utils.data import  TensorDataset, DataLoader\n",
    "dataset = TensorDataset(features, target)\n",
    "batch_size=64\n",
    "# Create a dataloader using the above dataset\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "validation_loss = 0.0\n",
    "validation_dataloader = # Some dataset portion\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    with torch.no_grad():\n",
    "        for data in validation_dataloader:\n",
    "            features , target = data\n",
    "            prediction = model(features)\n",
    "            loss = criterion(prediction, target)\n",
    "            validation_loss += loss.item()\n",
    "    validation_loss_epoch = validation_loss / len(dataloader)    \n",
    "```\n",
    "## Accuracy\n",
    "```python\n",
    "from torchmetrics import Accuracy\n",
    "metric = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "for epoch in range(num_epochs):\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            features, target = data\n",
    "            outputs = model(features)\n",
    "            acc = metric(outputs.softmax(dim=-1), target.argmax(dim=-1))  # If the target is OneHot represented [0,1,0]\n",
    "    # Calculate accuracy over the whole epoch\n",
    "    acc = metric.compute()\n",
    "    print(\"Accuracy in Epoch:{epoch}\",acc)\n",
    "    acc = metric.reset()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
